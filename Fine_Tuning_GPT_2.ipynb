{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction:**\n",
        "\n",
        "In this project, I've **fine-tuned a GPT-2 model** to generate conversational responses based on given instructions and context. Leveraging the power of natural language processing and deep learning, **the model is trained on a dataset containing input-output pairs**. To facilitate effective training, **I formated this dataset into instructional prompts paired with corresponding inputs and outputs.** Through preprocessing the data and employing techniques such as tokenization and language modeling, the model learns to generate coherent and contextually relevant responses to various tasks and inquiries. The trained model serves as a versatile and intelligent conversational agent capable of engaging in meaningful interactions across a wide range of scenarios. This project aims to showcase the capabilities of advanced language models in facilitating natural and fluid communication, paving the way for enhanced human-computer interaction and dialogue systems."
      ],
      "metadata": {
        "id": "CTSa2JpA0Hsh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6XyPUVGrMkF"
      },
      "outputs": [],
      "source": [
        "# These installations will depend on your environment.\n",
        "\n",
        "!pip install jsonlines\n",
        "!pip install datasets\n",
        "!pip install accelerate>=0.21.0\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports realted to the project\n",
        "\n",
        "import os\n",
        "\n",
        "import itertools # for iteration streaming datasets.\n",
        "import jsonlines\n",
        "\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling"
      ],
      "metadata": {
        "id": "J6UnuH99uDsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_and_process_dataset():\n",
        "    \"\"\"\n",
        "    Loads the dataset, processes it, and returns the processed data.\n",
        "\n",
        "    Returns:\n",
        "    processed_data (list): List of dictionaries containing processed prompts and outputs.\n",
        "    \"\"\"\n",
        "    dataset = load_dataset(\"yahma/alpaca-cleaned\", streaming=True, split='train')\n",
        "\n",
        "    prompt_template_with_input = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "    ### Instruction:\n",
        "    {instruction}\n",
        "\n",
        "    ### Input:\n",
        "    {input}\n",
        "\n",
        "    ### Response:\"\"\"\n",
        "\n",
        "    prompt_template_without_input = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "    ### Instruction:\n",
        "    {instruction}\n",
        "\n",
        "    ### Response:\"\"\"\n",
        "\n",
        "    processed_data = []\n",
        "    for entry in dataset:\n",
        "        if not entry[\"input\"]:\n",
        "            processed_prompt = prompt_template_without_input.format(instruction=entry[\"instruction\"])\n",
        "        else:\n",
        "            processed_prompt = prompt_template_with_input.format(instruction=entry[\"instruction\"], input=entry[\"input\"])\n",
        "\n",
        "        processed_data.append({\"input\": processed_prompt, \"output\": entry[\"output\"]})\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "def save_processed_data(processed_data, filename):\n",
        "    \"\"\"\n",
        "    Saves processed data to a JSONL file.\n",
        "\n",
        "    Args:\n",
        "    processed_data (list): List of dictionaries containing processed prompts and outputs.\n",
        "    filename (str): Name of the file to save.\n",
        "    \"\"\"\n",
        "    with jsonlines.open(filename, 'w') as writer:\n",
        "        writer.write_all(processed_data)\n",
        "\n",
        "def load_processed_dataset(filename):\n",
        "    \"\"\"\n",
        "    Loads processed dataset from a JSONL file.\n",
        "\n",
        "    Args:\n",
        "    filename (str): Name of the JSONL file.\n",
        "\n",
        "    Returns:\n",
        "    dataset (Dataset): Loaded dataset.\n",
        "    \"\"\"\n",
        "    dataset = load_dataset(\"json\", data_files=filename, split='train')\n",
        "    return dataset\n",
        "\n",
        "def preprocess_text(example):\n",
        "    \"\"\"\n",
        "    Tokenizes the text in an example.\n",
        "\n",
        "    Args:\n",
        "    example (dict): Input example containing text.\n",
        "\n",
        "    Returns:\n",
        "    dict: Tokenized example.\n",
        "    \"\"\"\n",
        "    return tokenizer(example['text'],\n",
        "                    padding=True,\n",
        "                    truncation=True,\n",
        "                    return_tensors='pt')\n",
        "\n",
        "def group_texts(examples):\n",
        "    \"\"\"\n",
        "    Groups text examples into chunks.\n",
        "\n",
        "    Args:\n",
        "    examples (dict): Dictionary containing input examples.\n",
        "\n",
        "    Returns:\n",
        "    dict: Grouped examples.\n",
        "    \"\"\"\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "\n",
        "    if total_length >= block_size:\n",
        "        total_length = (total_length // block_size) * block_size\n",
        "\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "\n",
        "def train_model(model, train_dataset, eval_dataset, data_collator, training_args):\n",
        "    \"\"\"\n",
        "    Trains the model.\n",
        "\n",
        "    Args:\n",
        "    model: The model to be trained.\n",
        "    train_dataset (Dataset): Training dataset.\n",
        "    eval_dataset (Dataset): Evaluation dataset.\n",
        "    data_collator: Data collator for language modeling.\n",
        "    training_args: Training arguments.\n",
        "    \"\"\"\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        data_collator=data_collator\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U17cELqFtaPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Load model and tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
        "\n",
        "    # Load and process dataset\n",
        "    processed_data = load_and_process_dataset()\n",
        "    save_processed_data(processed_data, \"alpaca_processed.jsonl\")\n",
        "\n",
        "    # Load processed dataset\n",
        "    dataset = load_processed_dataset(\"alpaca_processed.jsonl\")\n",
        "\n",
        "    # Tokenize dataset\n",
        "    block_size = 128\n",
        "    tokenized_dataset = dataset.map(preprocess_text, batched=True, remove_columns=dataset.column_names)\n",
        "\n",
        "    # Group texts\n",
        "    tokenized_dataset = tokenized_dataset.map(group_texts, batched=True)\n",
        "\n",
        "    # Split dataset\n",
        "    dataset = tokenized_dataset.train_test_split(test_size=0.2)\n",
        "    train_dataset = dataset['train']\n",
        "    val_dataset = dataset['test']\n",
        "\n",
        "    # Initialize data collator\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "\n",
        "    # set the path where you want to store model if you want to store in drive, you have to mount it first in google colab.\n",
        "    output_dir=\"/content/drive/MyDrive/gpt_conversation\",  # Directory to save model checkpoints and logs\n",
        "    overwrite_output_dir=True,  # Overwrite the content of the output directory if it exists\n",
        "\n",
        "    num_train_epochs=1,  # Total number of training epochs\n",
        "\n",
        "\n",
        "    per_device_train_batch_size=32,  # Batch size per GPU during training\n",
        "    per_device_eval_batch_size=32,  # Batch size per GPU during evaluation\n",
        "\n",
        "    learning_rate=2.0e-5,  # Learning rate for the optimizer\n",
        "    warmup_steps=1000,  # Number of steps for linear warmup\n",
        "    logging_dir=\"./logs\",  # Directory where logs will be saved\n",
        "    logging_steps=100,  # Log training metrics every X steps\n",
        "\n",
        "    evaluation_strategy=\"steps\",  # Evaluate model every `eval_steps`\n",
        "    eval_steps=2000,  # Evaluate model every X steps\n",
        "    save_strategy=\"steps\",  # Save model every `save_steps`\n",
        "    save_steps=300,  # Save model every X steps\n",
        "\n",
        "    save_total_limit=2,  # Limit the total number of saved checkpoints\n",
        "    report_to=\"wandb\",  # Enable logging to wandb\n",
        "    seed=42,  # Random seed for reproducibility\n",
        "    disable_tqdm=False  # Disable tqdm progress bars\n",
        ")\n",
        "     # Set wandb configuration\n",
        "    os.environ[\"WANDB_PROJECT\"] = \"Gpt_conversation\"\n",
        "    os.environ[\"WANDB_LOG_MODEL\"] = \"true\"\n",
        "    os.environ[\"WANDB_WATCH\"] = \"false\"\n",
        "\n",
        "    # Train the model\n",
        "   train_model(model, train_dataset, val_dataset, data_collator, training_args)\n"
      ],
      "metadata": {
        "id": "VDiLP0rZtvZy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}